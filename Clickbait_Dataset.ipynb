{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63aa837a-2455-4ba5-8114-fbb62e768f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN, LSTM, Flatten, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "203cd40d-ca64-49e4-b6c5-d2ed625fba98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline  clickbait\n",
      "0                                 Should I Get Bings          1\n",
      "1      Which TV Female Friend Group Do You Belong In          1\n",
      "2  The New \"Star Wars: The Force Awakens\" Trailer...          1\n",
      "3  This Vine Of New York On \"Celebrity Big Brothe...          1\n",
      "4  A Couple Did A Stunning Photo Shoot With Their...          1\n",
      "0                                                get bings\n",
      "1                            tv female friend group belong\n",
      "2          new star wars force awakens trailer give chills\n",
      "3        vine new york celebrity big brother fucking pe...\n",
      "4        couple stunning photo shoot baby learning inop...\n",
      "                               ...                        \n",
      "31995           make female hearts flutter iraq throw shoe\n",
      "31996    british liberal democrat patsy calton dies cancer\n",
      "31997    drone smartphone app help heart attack victims...\n",
      "31998    netanyahu urges pope benedict israel denounce ...\n",
      "31999    computer makers prepare stake bigger claim phones\n",
      "Name: clean_text, Length: 32000, dtype: object\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        1\n",
      "4        1\n",
      "        ..\n",
      "31995    0\n",
      "31996    0\n",
      "31997    0\n",
      "31998    0\n",
      "31999    0\n",
      "Name: clickbait, Length: 32000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('./clickbait_data.csv')\n",
    "print(df.head())\n",
    "\n",
    "# --- Cleaning functions ---\n",
    "def RemoveSpecialCharacters(sentence):\n",
    "    return re.sub('[^a-zA-Z]+',' ',sentence)\n",
    "\n",
    "def ConvertToLowerCase(sentence):\n",
    "    return sentence.lower()\n",
    "\n",
    "def CleanText(sentence):\n",
    "    sentence = str(sentence)\n",
    "    STOPWORDS = stopwords.words('english') + ['u','ü','ur','4','2','im','dont','doin','ure']\n",
    "    nopunc = [char for char in sentence if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    sentence = ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])\n",
    "    sentence = ConvertToLowerCase(RemoveSpecialCharacters(sentence))\n",
    "    return sentence\n",
    "\n",
    "# Apply cleaning\n",
    "df['clean_text'] = df['headline'].apply(CleanText)\n",
    "\n",
    "# Features and labels\n",
    "X = df['clean_text']\n",
    "y = df['clickbait']\n",
    "\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e5fb5cb-912b-4599-a07e-e07dccc1482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 25ms/step - accuracy: 0.8080 - loss: 0.4926 - val_accuracy: 0.9595 - val_loss: 0.3628\n",
      "Epoch 2/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.9796 - loss: 0.2840 - val_accuracy: 0.9634 - val_loss: 0.2680\n",
      "Epoch 3/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 26ms/step - accuracy: 0.9930 - loss: 0.1920 - val_accuracy: 0.9609 - val_loss: 0.2147\n",
      "Epoch 4/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 29ms/step - accuracy: 0.9971 - loss: 0.1364 - val_accuracy: 0.9605 - val_loss: 0.1832\n",
      "Epoch 5/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 23ms/step - accuracy: 0.9980 - loss: 0.1010 - val_accuracy: 0.9598 - val_loss: 0.1637\n",
      "ANN Accuracy: 0.9598437547683716\n"
     ]
    }
   ],
   "source": [
    "# Vectorize with TF-IDF\n",
    "vect = CountVectorizer()\n",
    "X_dtm = vect.fit_transform(X)\n",
    "tfidf = TfidfTransformer()\n",
    "X_tfidf = tfidf.fit_transform(X_dtm)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build ANN\n",
    "ann = Sequential()\n",
    "ann.add(Input(shape=(X_train.shape[1],)))\n",
    "ann.add(Dense(64, activation='relu'))\n",
    "ann.add(Dense(1, activation='sigmoid'))\n",
    "ann.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "ann_history = ann.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "ann_acc = ann.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"ANN Accuracy:\", ann_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "457fefac-e79d-40d5-a25f-2b1d8f36f2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 25ms/step - accuracy: 0.9016 - loss: 0.2302 - val_accuracy: 0.9541 - val_loss: 0.1216\n",
      "Epoch 2/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 22ms/step - accuracy: 0.9699 - loss: 0.0801 - val_accuracy: 0.9541 - val_loss: 0.1214\n",
      "Epoch 3/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 22ms/step - accuracy: 0.9849 - loss: 0.0427 - val_accuracy: 0.9516 - val_loss: 0.1387\n",
      "Epoch 4/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 22ms/step - accuracy: 0.9921 - loss: 0.0220 - val_accuracy: 0.9497 - val_loss: 0.1694\n",
      "Epoch 5/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step - accuracy: 0.9948 - loss: 0.0156 - val_accuracy: 0.9267 - val_loss: 0.2414\n",
      "RNN Accuracy: 0.9267187714576721\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=max_len)\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build RNN\n",
    "rnn = Sequential()\n",
    "rnn.add(Embedding(input_dim=max_words, output_dim=32))\n",
    "rnn.add(SimpleRNN(32))\n",
    "rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "rnn_history = rnn.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "rnn_acc = rnn.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"RNN Accuracy:\", rnn_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "841d2b15-d52a-4672-930f-28f3ef3de14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 33ms/step - accuracy: 0.9111 - loss: 0.2157 - val_accuracy: 0.9563 - val_loss: 0.1217\n",
      "Epoch 2/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 34ms/step - accuracy: 0.9669 - loss: 0.0850 - val_accuracy: 0.9548 - val_loss: 0.1229\n",
      "Epoch 3/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 36ms/step - accuracy: 0.9755 - loss: 0.0634 - val_accuracy: 0.9528 - val_loss: 0.1356\n",
      "Epoch 4/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 37ms/step - accuracy: 0.9795 - loss: 0.0518 - val_accuracy: 0.9519 - val_loss: 0.1448\n",
      "Epoch 5/5\n",
      "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.9823 - loss: 0.0435 - val_accuracy: 0.9475 - val_loss: 0.1636\n",
      "LSTM Accuracy: 0.9474999904632568\n"
     ]
    }
   ],
   "source": [
    "# Build LSTM\n",
    "lstm = Sequential()\n",
    "lstm.add(Embedding(input_dim=max_words, output_dim=32))\n",
    "lstm.add(LSTM(32))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "lstm_history = lstm.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate\n",
    "lstm_acc = lstm.evaluate(X_test, y_test, verbose=0)[1]\n",
    "print(\"LSTM Accuracy:\", lstm_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8af6c468-a575-4779-8a59-95a84216145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Comparison:\n",
      "ANN Accuracy:  0.9598\n",
      "RNN Accuracy:  0.9267\n",
      "LSTM Accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Comparison:\")\n",
    "print(f\"ANN Accuracy:  {ann_acc:.4f}\")\n",
    "print(f\"RNN Accuracy:  {rnn_acc:.4f}\")\n",
    "print(f\"LSTM Accuracy: {lstm_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb25df-c4c2-4185-b99d-394b9e63aa47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
